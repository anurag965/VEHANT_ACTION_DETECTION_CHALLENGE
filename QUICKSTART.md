# VEHANT Quick Reference Guide

Fast lookup for common commands and examples.

## üöÄ Installation (5 minutes)

```bash
# 1. Extract
unzip vehant_submission.zip
cd vehant_submission

# 2. Create environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: venv\Scripts\activate  # Windows

# 3. Install packages
pip install -r requirements.txt

# 4. Download pose model (4 MB)
mkdir -p convo
cd convo
wget https://storage.googleapis.com/mediapipe-tasks/python/pose_landmarker_lite.task
cd ..

# 5. Verify (optional but recommended)
python -c "from vehant_causal_temporal_model import VEHANTCausalTemporalModel; print('‚úì OK')"
```

**Result**: Environment ready for use

---

## üìπ Running Inference

### Basic Usage
```bash
python test.py --input_dir ./videos --output_file results.csv --threshold 0.5
```

### With Specific Model
```bash
python test.py --input_dir ./videos --output_file results.csv \
    --model_path models/causal_temporal/vehant_causal_temporal_finetuned.pth
```

### Batch Processing Multiple Directories
```bash
for dir in videos_*/; do
    python test.py --input_dir "$dir" --output_file "results_${dir%/}.csv"
done
```

---

## üéØ Output Format

**Input**: Folder of videos (`.mp4`, `.avi`, etc.)
**Output**: CSV file

```csv
video_name,class_id,x1,y1,x2,y2
fight_001.mp4,1,0.1234,0.2345,0.8765,0.9234
collapse_002.mp4,2,0.0500,0.1000,0.9500,0.8500
normal_003.mp4,0,0.0000,0.0000,1.0000,1.0000
```

**Coordinate System**:
- x1, y1 = top-left corner (normalized 0-1)
- x2, y2 = bottom-right corner (normalized 0-1)
- Multiply by frame width/height to get pixel coordinates

**Classes**:
- 0 = Negative (Normal, no action)
- 1 = Fight (Violent confrontation)
- 2 = Collapse (Person falling)

---

## üèãÔ∏è Training

### Train from Scratch
```bash
# Requires: dataset/fight_mp4s/, dataset/collapse_mp4s/, dataset/negatives/
python vehant_causal_temporal_model.py --stage train
```

### Fine-tune on Custom Data
```bash
# Requires: custom_data/fight/, custom_data/collapse/, custom_data/negative/
python vehant_causal_temporal_model.py --stage finetune
```

### Training Configuration (Edit in vehant_causal_temporal_model.py)
```python
class Config:
    BATCH_SIZE = 4                 # Videos per batch
    EPOCHS = 30                    # Training epochs
    LEARNING_RATE = 0.0001         # Initial LR
    PATIENCE = 10                  # Early stopping
    CONFIDENCE_THRESHOLD = 0.65    # Detection threshold
```

---

## ‚úÖ Validation

### Run Ablation Study
```bash
# Tests 5 model variants (RGB ‚Üí Full)
python ablation_study.py --dataset_path ./dataset --output ablation_results.json

# Expected results:
# Variant 1 (RGB): 87%
# Variant 2 (+ Motion): 89%
# Variant 3 (+ Causal): 91%
# Variant 4 (+ Uncertainty): 93%
# Variant 5 (Full): 95%
```

### Test Inference
```bash
# Create test video
python -c "
import cv2, numpy as np
out = cv2.VideoWriter('test.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (320,320))
for _ in range(30):
    out.write(np.random.randint(0, 255, (320,320,3), dtype=np.uint8))
out.release()
"

# Run inference
mkdir test_videos
mv test.mp4 test_videos/
python test.py --input_dir test_videos --output_file test_results.csv

# Check result
cat test_results.csv
```

---

## üîß Export & Deployment

### Convert to ONNX
```bash
python convert_pth_to_onnx.py --opset_version 14
```

### Quantized ONNX (4x smaller)
```bash
python convert_pth_to_onnx.py \
    --model_path models/causal_temporal/vehant_causal_temporal_original.pth \
    --output_path models/vehant_model_quantized.onnx \
    --quantize \
    --opset_version 14
```

### Android Optimized
```bash
python convert_pth_to_onnx.py \
    --model_path models/causal_temporal/vehant_causal_temporal_original.pth \
    --output_path models/vehant_android.onnx \
    --android_optimize \
    --opset_version 14
```

---

## ‚öôÔ∏è Configuration Tips

### For Limited Memory (GPU)
```python
class Config:
    BATCH_SIZE = 2              # Reduce from 4
    GRAD_ACCUMULATION_STEPS = 4 # Increase
    SEQUENCE_LENGTH = 8         # Reduce
```

### For CPU Only (No GPU)
```bash
CUDA_VISIBLE_DEVICES="" python test.py --input_dir videos --output_file results.csv
```

### For Speed (Lower Accuracy)
```python
class Config:
    CONFIDENCE_THRESHOLD = 0.5          # More detections
    SEQUENCE_LENGTH = 8                 # Shorter sequences
    IMG_SIZE = 224                      # Smaller frames
```

### For Accuracy (Lower Speed)
```python
class Config:
    CONFIDENCE_THRESHOLD = 0.8          # Fewer false positives
    SEQUENCE_LENGTH = 32                # More context
    IMG_SIZE = 512                      # Larger frames
```

---

## üêõ Troubleshooting

### No GPU Found
```bash
# Check GPU availability
python -c "import torch; print('GPU:', torch.cuda.is_available())"

# Force CPU
CUDA_VISIBLE_DEVICES="" python test.py ...

# Install NVIDIA drivers
nvidia-smi  # Should show GPU info
```

### Module Import Errors
```bash
# Reinstall everything
pip install --upgrade -r requirements.txt

# Check specific packages
python -c "import torch; import cv2; import mediapipe; print('‚úì All OK')"
```

### Out of Memory
```bash
# Reduce batch size
# Edit: BATCH_SIZE = 1

# Or use smaller sequences
# Edit: SEQUENCE_LENGTH = 8
```

### Slow Inference
```bash
# Check if using GPU
python -c "import torch; print('GPU:', torch.cuda.is_available())"

# If CPU, use ONNX quantized
python convert_pth_to_onnx.py --quantize

# Or reduce sequence length
# Edit: SEQUENCE_LENGTH = 8
```

### Pose Model Missing
```bash
mkdir -p convo
wget https://storage.googleapis.com/mediapipe-tasks/python/pose_landmarker_lite.task -O convo/pose_landmarker_lite.task
```

---

## üìä Performance Metrics

### Expected Results

| Device | Speed | Memory | Accuracy |
|--------|-------|--------|----------|
| RTX 3090 | 25 ms | 2 GB | 95% |
| A100 | 15 ms | 1.5 GB | 95% |
| CPU (i7) | 200 ms | 500 MB | 95% |
| Mobile ONNX | 500-800 ms | 200 MB | 95% |

### Model Size

| Format | Size |
|--------|------|
| PyTorch | 60 MB |
| ONNX | 55 MB |
| ONNX Quantized | 15 MB |

---

## üìö Documentation Guide

| Need | Read |
|------|------|
| Quick start | README.md |
| Installation | SETUP.md |
| Architecture | APPROACH.md |
| File listing | INDEX.md |
| This reference | QUICKSTART.md |

---

## üîë Key Concepts

### Classes
- **0: Negative** = Normal activity (no action of interest)
- **1: Fight** = Violent confrontation, assault
- **2: Collapse** = Person falling, medical emergency

### Modalities
- **RGB**: Color frames (320√ó320)
- **Optical Flow**: Motion between frames (64√ó64)
- **Pose**: Body joint locations (99-dim, MediaPipe)

### Loss Components
- **Classification**: Main task (weight: 2.0)
- **Bounding Box**: Object localization (weight: 0.3)
- **Temporal**: Action timing (weight: 0.5)
- **VQ Loss**: Motion tokenization (weight: 0.1)
- **Order Penalty**: Ensure onset < cessation (weight: 0.2)

### Uncertainty Types
- **Epistemic**: Model uncertainty (via MC Dropout)
- **Aleatoric**: Data uncertainty (learned)

---

## üí° Pro Tips

### 1. Use Confidence Threshold Tuning
```bash
# Precision-focused (few false positives)
python test.py --input_dir videos --output_file results.csv --threshold 0.8

# Recall-focused (catch more detections)
python test.py --input_dir videos --output_file results.csv --threshold 0.5
```

### 2. Batch Process Videos
```bash
# Create batch script
for video in videos/*.mp4; do
    dir=$(dirname "$video")
    python test.py --input_dir "$dir" --output_file "${video%.mp4}_result.csv"
done
```

### 3. Compare Models
```bash
# Original model
python test.py --input_dir videos --output_file results_original.csv \
    --model_path models/causal_temporal/vehant_causal_temporal_original.pth

# Fine-tuned model
python test.py --input_dir videos --output_file results_finetuned.csv \
    --model_path models/causal_temporal/vehant_causal_temporal_finetuned.pth
```

### 4. Monitor Training
```bash
# In another terminal
watch -n 5 "tail models/causal_temporal/*.log"
```

### 5. Generate Dataset Stats
```python
import os
from collections import Counter

dataset = []
for subdir in ['fight_mp4s', 'collapse_mp4s', 'negatives']:
    path = f'dataset/{subdir}'
    count = len([f for f in os.listdir(path) if f.endswith('.mp4')])
    dataset.append((subdir, count))

for name, count in dataset:
    print(f"{name}: {count} videos")
```

---

## üìû Common Commands Reference

```bash
# Install
pip install -r requirements.txt

# Verify
python -c "from vehant_causal_temporal_model import VEHANTCausalTemporalModel; print('‚úì')"

# Test inference
python test.py --input_dir test_videos --output_file results.csv

# Train
python vehant_causal_temporal_model.py --stage train

# Validate
python ablation_study.py --dataset_path dataset --output results.json

# Export
python convert_pth_to_onnx.py --model_path models/causal_temporal/vehant_causal_temporal_original.pth

# Check GPU
python -c "import torch; print('GPU Available:', torch.cuda.is_available())"

# See installed packages
pip list | grep -E "(torch|opencv|mediapipe)"

# Activate environment
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate     # Windows

# Deactivate environment
deactivate
```

---

## üéØ Typical Workflow

### Day 1: Setup & Test
```bash
# Setup (15 min)
pip install -r requirements.txt
mkdir -p convo && cd convo && wget ... && cd ..

# Test (5 min)
python test.py --input_dir test_videos --output_file results.csv
```

### Day 2+: Process Videos
```bash
# Prepare videos
mkdir my_videos && cp videos/* my_videos/

# Run inference
python test.py --input_dir my_videos --output_file my_results.csv

# Analyze results
head -20 my_results.csv
```

### Advanced: Fine-tune
```bash
# Prepare custom data
mkdir -p custom_data/{fight,collapse,negative}
# Add videos...

# Fine-tune
python vehant_causal_temporal_model.py --stage finetune

# Test fine-tuned model
python test.py --input_dir my_videos --output_file results_ft.csv \
    --model_path models/causal_temporal/vehant_causal_temporal_finetuned.pth
```

---

**Version**: 1.0  
**Last Updated**: January 2025  
**Status**: Production Ready

For detailed information, see README.md, SETUP.md, and APPROACH.md
